{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "d08cc895-f792-4c3f-a1ff-dcc8255ee6c7"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import Levenshtein as L\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "c113536e-11fd-44bc-acc0-bb8de09e037a"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import fuzz\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import skew, kurtosis\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data_emb', 'rb') as f:\n",
    "    all_sets, embs, word2idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {value: key for key, value in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = all_sets[0]\n",
    "val_sets = all_sets[1]\n",
    "test_sets = all_sets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = all_sets[0]['y'] + all_sets[1]['y']\n",
    "test_labels = all_sets[2]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_embedding(tokens):\n",
    "    embeddings = np.array([embs[t] for t in tokens])\n",
    "    if len(embeddings) == 0:\n",
    "        return np.repeat([1e-7], 300)\n",
    "    else:\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(data_sets):\n",
    "    feats = []\n",
    "    for q1_token, q2_token in tqdm(zip(data_sets['q1'], data_sets['q2']), total=len(data_sets['q1'])):\n",
    "        \n",
    "        q1 = \" \".join([idx2word[i] for i in q1_token])\n",
    "        q2 = \" \".join([idx2word[i] for i in q2_token])\n",
    "        q1 = re.sub(r' ([^a-z0-9])', r'\\1', q1)\n",
    "        q2 = re.sub(r' ([^a-z0-9])', r'\\1', q2)\n",
    "        q1_pos_tag = [k for t, k in pos_tag([idx2word[i] for i in q1_token])]\n",
    "        q2_pos_tag = [k for t, k in pos_tag([idx2word[i] for i in q2_token])]\n",
    "    \n",
    "        \n",
    "        len_q1 = len(q1)\n",
    "        len_q2 = len(q2)\n",
    "        diff_len = len_q1 - len_q2\n",
    "        num_char1 = len(set(q1))\n",
    "        num_char2 = len(set(q2))\n",
    "        num_token1 = len(q1_token)\n",
    "        num_token2 = len(q2_token)\n",
    "        num_common_tags = len(set(q1_pos_tag).intersection(set(q2_pos_tag)))\n",
    "        num_common_tokens = len(set(q1_token).intersection(set(q2_token)))\n",
    "        L_words_dist = L.distance(q1, q2)\n",
    "        L_tag_dist = L.distance(\" \".join(q1_pos_tag), \" \".join(q2_pos_tag))\n",
    "        \n",
    "        q_ratio = fuzz.QRatio(q1, q2)\n",
    "        wr_ratio = fuzz.WRatio(q1, q2)\n",
    "        partial_ratio = fuzz.partial_ratio(q1, q2)\n",
    "        token_set_ratio = fuzz.token_set_ratio(q1, q2)\n",
    "        token_sort_ratio = fuzz.token_sort_ratio(q1, q2)\n",
    "        partial_token_set_ratio = fuzz.partial_token_set_ratio(q1, q2)\n",
    "        partial_token_sort_ratio = fuzz.partial_token_sort_ratio(q1, q2)\n",
    "        \n",
    "        q1_embedding = average_embedding(q1_token)\n",
    "        q2_embedding = average_embedding(q2_token)\n",
    "        \n",
    "        #cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "        embedding_cos_sim = cosine(q1_embedding, q2_embedding)\n",
    "        embedding_euclidean = euclidean(q1_embedding, q2_embedding)\n",
    "        embedding_jaccard = jaccard(q1_embedding, q2_embedding)\n",
    "        embedding_city = cityblock(q1_embedding, q2_embedding)\n",
    "        embedding_canberra = canberra(q1_embedding, q2_embedding)\n",
    "        embedding_minkowski = minkowski(q1_embedding, q2_embedding, 3)\n",
    "        embeeding_braycurtis = braycurtis(q1_embedding, q2_embedding)\n",
    "        \n",
    "        q1_kur = kurtosis(q1_embedding)\n",
    "        q2_kur = kurtosis(q2_embedding)\n",
    "        q1_skew = skew(q1_embedding)\n",
    "        q2_skew = skew(q2_embedding)\n",
    "        \n",
    "        feats.append([len_q1, len_q2, diff_len,\n",
    "                      num_char1, num_char2, num_token1, num_token2,\n",
    "                      num_common_tags, num_common_tokens,\n",
    "                      embedding_cos_sim, embedding_euclidean, embedding_jaccard, embedding_city,\n",
    "                      embedding_canberra, embedding_minkowski, embeeding_braycurtis,\n",
    "                      L_words_dist, L_tag_dist,\n",
    "                      q_ratio, wr_ratio, partial_ratio, token_set_ratio, token_sort_ratio,\n",
    "                      partial_token_set_ratio, partial_token_sort_ratio,\n",
    "                      q1_kur, q2_kur, q1_skew, q2_skew] + q1_embedding.tolist() + q2_embedding.tolist())\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 323480/323480 [19:15<00:00, 279.86it/s]\n",
      "100%|██████████| 40435/40435 [02:24<00:00, 279.56it/s]\n"
     ]
    }
   ],
   "source": [
    "train_feats = extract_feature(train_sets) + extract_feature(val_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40436/40436 [02:24<00:00, 279.38it/s]\n"
     ]
    }
   ],
   "source": [
    "test_feats = extract_feature(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n"
     ]
    }
   ],
   "source": [
    "print(len(train_feats[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = np.array(train_feats)\n",
    "test_feats = np.array(test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_train_feats = train_feats[:, 29:]\n",
    "partial_test_feats = test_feats[:, 29:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(partial_test_feats[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest = RandomForestClassifier(max_depth=5, n_estimators=100, max_features=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest.fit(partial_train_feats, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest.score(partial_test_feats, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "    \n",
    "    experiment #1\n",
    "    max_depth = 5, n_estimators=100, max_features=7\n",
    "    without 2 x 300 word2vec features\n",
    "    test accuracy: 0.7118409338213473\n",
    "    \n",
    "    experiment #2\n",
    "    max_depth = 5, n_estimators=100, max_features=7\n",
    "    with 2 x 300 word2vec features\n",
    "    test accuracy: 0.651498664556336\n",
    "    \n",
    "    experiment #3\n",
    "    max_depth = 5, n_estimators=100, max_features=15\n",
    "    without 2 x 300 word2vec features\n",
    "    test accuracy: 0.7117172816302305\n",
    "    \n",
    "    experiment #4\n",
    "    max_depth = 5, n_estimators=100, max_features=15\n",
    "    2 x 300 word2vec features ONLY\n",
    "    test accuracy:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ee4c4ada-173b-4aaa-a72f-abdbf5431dae"
    }
   },
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(\"questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cae95f3c-9f38-4320-b8f5-d837d2db375b"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "961ccd87-3ded-45ce-8f3d-8b7dbec1c71c"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f6433637-c871-42ee-8251-76be2593c456"
    }
   },
   "outputs": [],
   "source": [
    "original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "47ffdd79-7778-4419-aa28-ff4805de68b8"
    }
   },
   "outputs": [],
   "source": [
    "print(len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bcd1bbbf-8482-435f-b8f8-93b29d0344b9"
    }
   },
   "outputs": [],
   "source": [
    "original_data.dropna(axis=0, how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a8b5cd41-286b-43c2-bc54-e816c645bcaa"
    }
   },
   "outputs": [],
   "source": [
    "print(len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f0e91aba-8155-4d01-b215-0564dc21128c"
    }
   },
   "outputs": [],
   "source": [
    "all_label = original_data.is_duplicate.tolist()\n",
    "all_data = original_data[['question1', 'question2']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fabc4d29-57c1-4e35-8d2b-786ff39f3281"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(all_data, all_label, \n",
    "                                                                  test_size=0.1, stratify=all_label,\n",
    "                                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "dd6047b2-2cbf-4449-b88a-ab8c2c3deaa4"
    }
   },
   "outputs": [],
   "source": [
    "#train_data, train_label = all_data, all_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "83fde3bd-9edb-44a7-87cb-acba1ff74ace"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"train_data.npy\", train_data)\n",
    "np.save(\"test_data.npy\", test_data)\n",
    "np.save(\"train_label.npy\", train_label)\n",
    "np.save(\"test_label.npy\", test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "375d5aa5-5f20-464b-98f2-a102608c1789"
    }
   },
   "outputs": [],
   "source": [
    "def sentence2vec(s):\n",
    "    words = s.lower()\n",
    "    words = word_tokenize(words)\n",
    "\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(M) == 0:\n",
    "        return np.repeat(1e-7, 300)\n",
    "    else:\n",
    "        \n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis=0)\n",
    "        return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7fd4d31d-946f-4cf8-9e37-01fa27b8c72e"
    }
   },
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a11ca6ab-fbc7-48b9-9a7e-dde859ecf9b7"
    }
   },
   "outputs": [],
   "source": [
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "95f644c4-36f0-43c2-9202-00bdcc5569de"
    }
   },
   "outputs": [],
   "source": [
    "def feature_extract(data):\n",
    "    feats = []\n",
    "    for i, (q1, q2) in enumerate(tqdm(data)):\n",
    "        #try:\n",
    "        q1_token = word_tokenize(q1)\n",
    "        q2_token = word_tokenize(q2)\n",
    "        q1_pos_tag = [k for t, k in pos_tag(q1_token)]\n",
    "        q2_pos_tag = [k for t, k in pos_tag(q2_token)]\n",
    "        #print(q1pos_tag)\n",
    "        #print(q2pos_tag)\n",
    "            \n",
    "#         q1_embedding = []\n",
    "#         for t in q1_token:\n",
    "#             try:\n",
    "#                 q1_embedding.append(model[t])\n",
    "#             except:\n",
    "#                 continue\n",
    "#         q2_embedding = []\n",
    "#         for t in q2_token:\n",
    "#             try:\n",
    "#                 q2_embedding.append(model[t])\n",
    "#             except:\n",
    "#                 continue\n",
    "                    \n",
    "#         q1_embedding = np.mean(np.array(np.nan_to_num(q1_embedding)), axis=0)\n",
    "#         q2_embedding = np.mean(np.array(np.nan_to_num(q2_embedding)), axis=0)\n",
    "#         q1_embedding = np.nan_to_num(q1_embedding)\n",
    "#         q2_embedding = np.nan_to_num(q2_embedding)\n",
    "        \n",
    "        #q1_embedding = np.nan_to_num(sentence2vec(q1))\n",
    "        #q2_embedding = np.nan_to_num(sentence2vec(q2))\n",
    "        q1_embedding = q1_embeddings[i]\n",
    "        q2_embedding = q2_embeddings[i]\n",
    "        \n",
    "        len_q1 = len(q1)\n",
    "        len_q2 = len(q2)\n",
    "        num_char1 = len(set(q1))\n",
    "        num_char2 = len(set(q2))\n",
    "        len_q1_token = len(q1_token)\n",
    "        len_q2_token = len(q2_token)\n",
    "        diff_len = len_q1 - len_q2\n",
    "        num_common_tags = len(set(q1_pos_tag).intersection(set(q2_pos_tag)))\n",
    "        num_common_words = len(set(q1.lower().split(' ')).intersection(set(q2.lower().split(' '))))\n",
    "            \n",
    "        L_words_dist = L.distance(q1, q2)\n",
    "        L_tag_dist = L.distance(\" \".join(q1_pos_tag), \" \".join(q2_pos_tag))\n",
    "        wmd_dist = wmd(q1, q2)\n",
    "        norm_wmd_dist = norm_wmd(q1, q2)\n",
    "            \n",
    "        #cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "        embedding_cos_sim = cosine(q1_embedding, q2_embedding)\n",
    "        embedding_euclidean = euclidean(q1_embedding, q2_embedding)\n",
    "        embedding_jaccard = jaccard(q1_embedding, q2_embedding)\n",
    "        embedding_city = cityblock(q1_embedding, q2_embedding)\n",
    "        embedding_canberra = canberra(q1_embedding, q2_embedding)\n",
    "        embedding_mink = minkowski(q1_embedding, q2_embedding, 3)\n",
    "        embeeding_braycurtis = braycurtis(q1_embedding, q2_embedding)\n",
    "                \n",
    "        q_ratio = fuzz.QRatio(q1, q2)\n",
    "        wr_ratio = fuzz.WRatio(q1, q2)\n",
    "        partial_ratio = fuzz.partial_ratio(q1, q2)\n",
    "        token_set_ratio = fuzz.token_set_ratio(q1, q2)\n",
    "        token_sort_ratio = fuzz.token_sort_ratio(q1, q2)\n",
    "        partial_token_set_ratio = fuzz.partial_token_set_ratio(q1, q2)\n",
    "        partial_token_sort_ratio = fuzz.partial_token_sort_ratio(q1, q2)\n",
    "        \n",
    "        q1_kur = kurtosis(q1_embedding)\n",
    "        q2_kur = kurtosis(q2_embedding)\n",
    "        q1_skew = skew(q1_embedding)\n",
    "        q2_skew = skew(q2_embedding)\n",
    "        \n",
    "        feats.append([len_q1, len_q2, num_char1, num_char2, len_q1_token, len_q2_token,\n",
    "                      num_common_tags, num_common_words, diff_len,\n",
    "                      embedding_cos_sim, embedding_euclidean, embedding_jaccard, embedding_city,\n",
    "                      embedding_canberra, embedding_mink, embeeding_braycurtis,\n",
    "                      L_words_dist, L_tag_dist,# wmd_dist, norm_wmd_dist,\n",
    "                      q_ratio, wr_ratio, partial_ratio, token_set_ratio, token_sort_ratio,\n",
    "                      partial_token_set_ratio, partial_token_sort_ratio,\n",
    "                      q1_kur, q2_kur, q1_skew, q2_skew] + q1_embedding.tolist() + q2_embedding.tolist()) \n",
    "        #except:\n",
    "        #    print(q1)\n",
    "        #    print(q2)\n",
    "\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8ee7b4d1-73ca-4b7b-802a-9d44b0b1ef6b"
    }
   },
   "outputs": [],
   "source": [
    "q1_embeddings = [sentence2vec(q1) for q1, _ in train_data]\n",
    "q2_embeddings = [sentence2vec(q2) for _, q2 in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f73df73c-4062-4389-ae17-4a5172d71b35"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"q1_w2v.npy\", q1_embeddings)\n",
    "np.save(\"q2_w2v.npy\", q2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d35f05f9-8a44-489a-8bb5-83ccbecf01b6"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0afaee1d-37f5-44ad-bc9f-07e5826272b7"
    }
   },
   "outputs": [],
   "source": [
    "train_feats = feature_extract(train_data)\n",
    "test_feats = feature_extract(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "39689aa1-497e-4e12-a33a-5731aa509fd0"
    }
   },
   "outputs": [],
   "source": [
    "np.any(np.isnan(train_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "46f73ec2-31ac-4a73-b56c-02111c78c162"
    }
   },
   "outputs": [],
   "source": [
    "np.any(np.isfinite(train_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "851bc392-46af-4192-b10f-e57d7be588b4"
    }
   },
   "outputs": [],
   "source": [
    "train_feats[train_feats > 1000] = 1e-7\n",
    "test_feats[test_feats > 1000] = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a1538d09-3fed-4991-b959-78d79b509d30"
    }
   },
   "outputs": [],
   "source": [
    "RandomForest = RandomForestClassifier(max_depth=5, n_estimators=100, max_features=7)\n",
    "RandomForest.fit(train_feats, train_label)\n",
    "RandomForest.score(test_feats, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9421638a-cf32-4d54-982a-7539d2e5f052"
    }
   },
   "outputs": [],
   "source": [
    "RandomForest.score(test_feats, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "22cb15cc-b1e9-4883-ad0e-d8de7280ca6f"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_feats[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6f45adbc-934b-4555-a178-219027098ef3"
    }
   },
   "outputs": [],
   "source": [
    "for i, (q1, q2) in enumerate(tqdm(train_data)):\n",
    "    if \"What would happen if the Indian government\" in q1 or \"What would happen if the Indian government\" in q2:\n",
    "        print(i)\n",
    "        print(q1)\n",
    "        print(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "92c0ca94-2629-4460-8da3-58df4a3580da"
    }
   },
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "partial_train_feats = min_max_scaler.fit_transform(train_feats[:, :25])\n",
    "partial_test_feats = min_max_scaler.fit_transform(test_feats[:, :25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "436f4148-f462-463c-8d7e-85e82b6192ad"
    }
   },
   "outputs": [],
   "source": [
    "RandomForest.fit(partial_train_feats, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0e68543e-3124-4b07-9aa1-87f4661a5456"
    }
   },
   "outputs": [],
   "source": [
    "RandomForest.score(partial_test_feats, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5fab09ff-6f19-4b56-9934-d81850989ed8"
    }
   },
   "outputs": [],
   "source": [
    "RandomForest = RandomForestClassifier(max_depth=5, n_estimators=100, max_features=150)\n",
    "RandomForest.fit(train_feats, train_label)\n",
    "RandomForest.score(test_feats, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fd2a209f-b336-47a8-bdcf-506eaa0b2a2b"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
